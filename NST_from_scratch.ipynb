{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JSeVEVFeW1Ei"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from PIL import Image\n",
        "import torchvision.transforms as transforms#img to tensor\n",
        "import torchvision.models as models#vgg19\n",
        "from torchvision.utils import save_image"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UfG4NzG0XP3S",
        "outputId": "db63cbf6-593b-43ad-cef0-52a394a4ca0b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=VGG19_Weights.IMAGENET1K_V1`. You can also use `weights=VGG19_Weights.DEFAULT` to get the most up-to-date weights.\n",
            "  warnings.warn(msg)\n",
            "Downloading: \"https://download.pytorch.org/models/vgg19-dcbb9e9d.pth\" to /root/.cache/torch/hub/checkpoints/vgg19-dcbb9e9d.pth\n",
            "100%|██████████| 548M/548M [00:06<00:00, 84.0MB/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Sequential(\n",
            "  (0): Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "  (1): ReLU(inplace=True)\n",
            "  (2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "  (3): ReLU(inplace=True)\n",
            "  (4): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
            "  (5): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "  (6): ReLU(inplace=True)\n",
            "  (7): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "  (8): ReLU(inplace=True)\n",
            "  (9): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
            "  (10): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "  (11): ReLU(inplace=True)\n",
            "  (12): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "  (13): ReLU(inplace=True)\n",
            "  (14): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "  (15): ReLU(inplace=True)\n",
            "  (16): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "  (17): ReLU(inplace=True)\n",
            "  (18): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
            "  (19): Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "  (20): ReLU(inplace=True)\n",
            "  (21): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "  (22): ReLU(inplace=True)\n",
            "  (23): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "  (24): ReLU(inplace=True)\n",
            "  (25): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "  (26): ReLU(inplace=True)\n",
            "  (27): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
            "  (28): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "  (29): ReLU(inplace=True)\n",
            "  (30): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "  (31): ReLU(inplace=True)\n",
            "  (32): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "  (33): ReLU(inplace=True)\n",
            "  (34): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "  (35): ReLU(inplace=True)\n",
            "  (36): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
            ")\n"
          ]
        }
      ],
      "source": [
        "model=models.vgg19(pretrained=True).features\n",
        "print(model)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SP0hZFbtXhew"
      },
      "outputs": [],
      "source": [
        "#needed layers=0,5,10,19,28\n",
        "class VGG(nn.Module):\n",
        "  def __init__(self):\n",
        "    super(VGG, self).__init__()\n",
        "\n",
        "    self.chosen_features = ['0', '5', '10', '19', '28']\n",
        "    self.model = models.vgg19(pretrained=True).features[:29]\n",
        "\n",
        "  def forward(self, x):\n",
        "    features = []\n",
        "    for layer_num, layer in enumerate(self.model):\n",
        "      x = layer(x)\n",
        "      if str(layer_num) in self.chosen_features:\n",
        "        features.append(x)\n",
        "    return features\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kZJ1GlKlZxs1"
      },
      "outputs": [],
      "source": [
        "def load_image(image_name):\n",
        "  image=Image.open(image_name)\n",
        "  image=loader(image).unsqueeze(0)\n",
        "  return image\n",
        "\n",
        "#device=torch.device(\"cude\" if torch.cuda is_available else \"cpu\")\n",
        "image_size=400\n",
        "\n",
        "loader=transforms.Compose(\n",
        "    [\n",
        "        transforms.Resize([image_size,image_size]),\n",
        "        transforms.ToTensor(),\n",
        "        #transforms.Normalize(mean=[0.485, 0.456, 0.406],std=[0.229, 0.224, 0.225])\n",
        "    ]\n",
        ")\n",
        "\n",
        "original_img=load_image(\"/content/1.jpeg\")\n",
        "style_img=load_image(\"/content/2.jpg\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tZKzye_pcleZ",
        "outputId": "98a192ad-d42c-4f52-c924-fe3952c03311"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=VGG19_Weights.IMAGENET1K_V1`. You can also use `weights=VGG19_Weights.DEFAULT` to get the most up-to-date weights.\n",
            "  warnings.warn(msg)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Step 0: Total Loss: 1091594.75\n",
            "Step 100: Total Loss: 190620.921875\n",
            "Step 200: Total Loss: 94442.6484375\n",
            "Step 300: Total Loss: 58888.6328125\n",
            "Step 400: Total Loss: 41855.296875\n",
            "Step 500: Total Loss: 31031.19140625\n"
          ]
        }
      ],
      "source": [
        "#3 inputs= style image,contentimg,random noise as generated image and then optmize the random image\n",
        "#generated=torch.randn(original_image.shape,requires_grad=True)\n",
        "#started with a clone of org image instead of random noise gives better results\n",
        "#its faster\n",
        "model=VGG().eval()\n",
        "generated=original_img.clone().requires_grad_(True)\n",
        "\n",
        "#hp\n",
        "total_steps=6000\n",
        "lr=0.001\n",
        "alpha=1\n",
        "beta=0.01\n",
        "\n",
        "optimizer=optim.AdamW([generated],lr=lr)\n",
        "\n",
        "for step in range(total_steps):\n",
        "  #1.send the 3 images to vgg network\n",
        "  generated_features=model(generated)\n",
        "  original_img_features=model(original_img)\n",
        "  style_features=model(style_img)\n",
        "\n",
        "  style_loss=original_loss=0\n",
        "\n",
        "  for gen_feature,orig_feature,style_feature in zip(\n",
        "      generated_features,original_img_features,style_features\n",
        "  ):\n",
        "      batch_size,channel,height,width=gen_feature.shape\n",
        "      original_loss+=torch.mean((gen_feature-orig_feature)**2)\n",
        "\n",
        "      #gram matrix\n",
        "      #for generated and the style\n",
        "      G=gen_feature.view(channel,height*width).mm(\n",
        "          gen_feature.view(channel,height*width).t()\n",
        "      )\n",
        "\n",
        "      A=style_feature.view(channel,height*width).mm(\n",
        "          style_feature.view(channel,height*width).t()\n",
        "      )\n",
        "\n",
        "      style_loss+=torch.mean((G-A)**2)\n",
        "\n",
        "  total_loss=alpha*original_loss +beta*style_loss\n",
        "  optimizer.zero_grad()\n",
        "  total_loss.backward()\n",
        "  optimizer.step()\n",
        "\n",
        "  if step % 100 == 0:\n",
        "    print(f\"Step {step}: Total Loss: {total_loss.item()}\")\n",
        "    save_image(generated, f\"generated_{step}.jpeg\")\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "jd324PdsjHbu"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}